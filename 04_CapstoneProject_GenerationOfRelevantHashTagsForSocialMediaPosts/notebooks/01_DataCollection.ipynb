{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "#### **Overview of Objectives**\n",
    "- Extract top trending hashtags from GetDayTrends for the United States.\n",
    "- Fetch tweets associated with each hashtag using the Twitter API.\n",
    "- Apply preprocessing to filter English tweets and remove duplicates.\n",
    "- Store results in structured CSV files for individual hashtags and a consolidated dataset.\n",
    "\n",
    "#### Requirements\n",
    "    1. Update username, password and email id in the [config.ini file](../config.ini) before running the code.\n",
    "#### Expectations\n",
    "    2. It will generate some csv files in the [data](./data/) folder. Each hash tag will have its own csv file at the end of this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step-by-Step Methodology**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Scraping Trending Hashtags**\n",
    "   - **Tools Used**:\n",
    "     - `BeautifulSoup` for HTML parsing.\n",
    "     - `urllib` for fetching the webpage content.\n",
    "\n",
    "   - **Process**:\n",
    "     - Top hashtags were extracted from two sections of the GetDayTrends webpage:\n",
    "       - Most Tweeted Hashtags.\n",
    "       - Longest-Trending Hashtags.\n",
    "     - HTML tags and CSS classes were used to locate and parse hashtags.\n",
    "\n",
    "   - **Output**:\n",
    "     - A combined, de-duplicated list of hashtags ready for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "def extract_tags(webpage):\n",
    "    '''\n",
    "    Extracts the top hashtags from a webpage\n",
    "    The code is specific to the structure of the webpage\n",
    "    '''\n",
    "    soup = BeautifulSoup(webpage, 'html.parser')\n",
    "\n",
    "    # Find the section containing the top hashtags\n",
    "    hashtags_section = soup.find('table', class_='ranking')\n",
    "\n",
    "    all_hashtags = []\n",
    "    # Extract and print each hashtag\n",
    "    if hashtags_section:\n",
    "        hashtags = hashtags_section.find_all('a')\n",
    "        for hashtag in hashtags:\n",
    "            all_hashtags.append(hashtag.text)\n",
    "    return all_hashtags\n",
    "\n",
    "\n",
    "def get_tags_from_url(url):\n",
    "    '''\n",
    "    Extracts the top hashtags from a webpage\n",
    "    '''\n",
    "    # Send a GET request to the URL\n",
    "    response = Request(url, headers={'User-Agent': 'insomnia/2023.5.8'})\n",
    "    webpage = urlopen(response).read()\n",
    "    return extract_tags(webpage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#AskAiah',\n",
       " '#Survivor47',\n",
       " '#PlayStationWrapUp2024',\n",
       " '#AEWDynamite',\n",
       " '#ThursdayMotivation',\n",
       " '#RHOSLC',\n",
       " '#CUTOSHI',\n",
       " '#thursdayvibes',\n",
       " '#GSWvsHOU',\n",
       " '#liftoff',\n",
       " '#PMSLive',\n",
       " '#BBMAs',\n",
       " '#ThursdayThoughts',\n",
       " '#playstationwrapup',\n",
       " '#TheGameAwards',\n",
       " '#WhyIChime',\n",
       " '#TNFonPrime']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# URL of the target page\n",
    "url_most_tweeted = 'https://getdaytrends.com/united-states/top/tweeted/day/'\n",
    "url_longest_trending = 'https://getdaytrends.com/united-states/top/longest/day/'\n",
    "\n",
    "\n",
    "most_tweeted_hashtags = get_tags_from_url(url_most_tweeted)\n",
    "longest_trending_hashtags = get_tags_from_url(url_longest_trending)\n",
    "\n",
    "(most_tweeted_hashtags, longest_trending_hashtags)\n",
    "\n",
    "# combine the two lists and remove duplicates\n",
    "all_hashtags = list(set(most_tweeted_hashtags + longest_trending_hashtags))\n",
    "all_hashtags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Fetching Tweets**\n",
    "   - **Tools Used**:\n",
    "     - `twikit` library for Twitter API interactions.\n",
    "     - `random`, `time` for implementing delays to respect API rate limits.\n",
    "\n",
    "   - **Process**:\n",
    "     - Authenticated using credentials stored in a configuration file.\n",
    "        You need to place your username, email and password in the [config.ini file](../config.ini)\n",
    "     - For each hashtag:\n",
    "       - Top 50 tweets were fetched.\n",
    "       - Tweets were stored in CSV files, one per hashtag.\n",
    "       - A random delay between 20â€“40 seconds was employed to avoid rate-limit violations.\n",
    "\n",
    "   - **Output**:\n",
    "     - CSV files containing tweets, hashtags, and language metadata for each hashtag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Waiting for 25 seconds before fetching tweets for #AskAiah...\n",
      "Getting tweets for #AskAiah\n",
      "Got 19 tweets for #AskAiah. Getting more after 15 seconds...\n",
      "Got 39 tweets for #AskAiah. Getting more after 6 seconds...\n",
      "Got 59 tweets for #AskAiah\n",
      "Got 41 unique tweets for #AskAiah from 59\n",
      "1. Waiting for 39 seconds before fetching tweets for #Survivor47...\n",
      "Getting tweets for #Survivor47\n",
      "Got 19 tweets for #Survivor47. Getting more after 15 seconds...\n",
      "Got 39 tweets for #Survivor47. Getting more after 6 seconds...\n",
      "Got 59 tweets for #Survivor47\n",
      "Got 39 unique tweets for #Survivor47 from 59\n",
      "2. Waiting for 35 seconds before fetching tweets for #PlayStationWrapUp2024...\n",
      "Getting tweets for #PlayStationWrapUp2024\n",
      "Got 19 tweets for #PlayStationWrapUp2024. Getting more after 17 seconds...\n",
      "Got 39 tweets for #PlayStationWrapUp2024. Getting more after 13 seconds...\n",
      "Got 59 tweets for #PlayStationWrapUp2024\n",
      "Got 45 unique tweets for #PlayStationWrapUp2024 from 59\n",
      "3. Waiting for 20 seconds before fetching tweets for #AEWDynamite...\n",
      "Getting tweets for #AEWDynamite\n",
      "Got 19 tweets for #AEWDynamite. Getting more after 17 seconds...\n",
      "Got 39 tweets for #AEWDynamite. Getting more after 20 seconds...\n",
      "Got 59 tweets for #AEWDynamite\n",
      "Got 41 unique tweets for #AEWDynamite from 59\n",
      "4. Waiting for 40 seconds before fetching tweets for #ThursdayMotivation...\n",
      "Getting tweets for #ThursdayMotivation\n",
      "Got 19 tweets for #ThursdayMotivation. Getting more after 17 seconds...\n",
      "Got 39 tweets for #ThursdayMotivation. Getting more after 10 seconds...\n",
      "Got 59 tweets for #ThursdayMotivation\n",
      "Got 46 unique tweets for #ThursdayMotivation from 59\n",
      "5. Waiting for 40 seconds before fetching tweets for #RHOSLC...\n",
      "Getting tweets for #RHOSLC\n",
      "Got 19 tweets for #RHOSLC. Getting more after 11 seconds...\n",
      "Got 39 tweets for #RHOSLC. Getting more after 5 seconds...\n",
      "Got 59 tweets for #RHOSLC\n",
      "Got 40 unique tweets for #RHOSLC from 59\n",
      "6. Waiting for 21 seconds before fetching tweets for #CUTOSHI...\n",
      "Getting tweets for #CUTOSHI\n",
      "Got 19 tweets for #CUTOSHI. Getting more after 11 seconds...\n",
      "Got 39 tweets for #CUTOSHI. Getting more after 16 seconds...\n",
      "Got 59 tweets for #CUTOSHI\n",
      "Got 39 unique tweets for #CUTOSHI from 59\n",
      "7. Waiting for 39 seconds before fetching tweets for #thursdayvibes...\n",
      "Getting tweets for #thursdayvibes\n",
      "Got 19 tweets for #thursdayvibes. Getting more after 9 seconds...\n",
      "Got 39 tweets for #thursdayvibes. Getting more after 11 seconds...\n",
      "Got 59 tweets for #thursdayvibes\n",
      "Got 42 unique tweets for #thursdayvibes from 59\n",
      "8. Waiting for 27 seconds before fetching tweets for #GSWvsHOU...\n",
      "Getting tweets for #GSWvsHOU\n",
      "Got 20 tweets for #GSWvsHOU. Getting more after 14 seconds...\n",
      "Got 40 tweets for #GSWvsHOU. Getting more after 12 seconds...\n",
      "Got 60 tweets for #GSWvsHOU\n",
      "Got 40 unique tweets for #GSWvsHOU from 60\n",
      "9. Waiting for 27 seconds before fetching tweets for #liftoff...\n",
      "Getting tweets for #liftoff\n",
      "Got 19 tweets for #liftoff. Getting more after 17 seconds...\n",
      "Got 39 tweets for #liftoff. Getting more after 14 seconds...\n",
      "Got 59 tweets for #liftoff\n",
      "Got 40 unique tweets for #liftoff from 59\n",
      "10. Waiting for 33 seconds before fetching tweets for #PMSLive...\n",
      "Getting tweets for #PMSLive\n",
      "Got 19 tweets for #PMSLive. Getting more after 16 seconds...\n",
      "Got 39 tweets for #PMSLive. Getting more after 16 seconds...\n",
      "Got 59 tweets for #PMSLive\n",
      "Got 40 unique tweets for #PMSLive from 59\n",
      "11. Waiting for 23 seconds before fetching tweets for #BBMAs...\n",
      "Getting tweets for #BBMAs\n",
      "Got 19 tweets for #BBMAs. Getting more after 18 seconds...\n",
      "Got 39 tweets for #BBMAs. Getting more after 8 seconds...\n",
      "Got 59 tweets for #BBMAs\n",
      "Got 50 unique tweets for #BBMAs from 59\n",
      "12. Waiting for 39 seconds before fetching tweets for #ThursdayThoughts...\n",
      "Getting tweets for #ThursdayThoughts\n",
      "Got 19 tweets for #ThursdayThoughts. Getting more after 19 seconds...\n",
      "Got 39 tweets for #ThursdayThoughts. Getting more after 17 seconds...\n",
      "Got 59 tweets for #ThursdayThoughts\n",
      "Got 40 unique tweets for #ThursdayThoughts from 59\n",
      "13. Waiting for 33 seconds before fetching tweets for #playstationwrapup...\n",
      "Getting tweets for #playstationwrapup\n",
      "Got 19 tweets for #playstationwrapup. Getting more after 15 seconds...\n",
      "Got 39 tweets for #playstationwrapup. Getting more after 18 seconds...\n",
      "Got 59 tweets for #playstationwrapup\n",
      "Got 40 unique tweets for #playstationwrapup from 59\n",
      "14. Waiting for 26 seconds before fetching tweets for #TheGameAwards...\n",
      "Getting tweets for #TheGameAwards\n",
      "Got 19 tweets for #TheGameAwards. Getting more after 6 seconds...\n",
      "Got 39 tweets for #TheGameAwards. Getting more after 16 seconds...\n",
      "Got 59 tweets for #TheGameAwards\n",
      "Got 40 unique tweets for #TheGameAwards from 59\n",
      "15. Waiting for 24 seconds before fetching tweets for #WhyIChime...\n",
      "Getting tweets for #WhyIChime\n",
      "Got 19 tweets for #WhyIChime. Getting more after 13 seconds...\n",
      "Got 39 tweets for #WhyIChime. Getting more after 19 seconds...\n",
      "Got 59 tweets for #WhyIChime\n",
      "Got 39 unique tweets for #WhyIChime from 59\n",
      "16. Waiting for 23 seconds before fetching tweets for #TNFonPrime...\n",
      "Getting tweets for #TNFonPrime\n",
      "Got 19 tweets for #TNFonPrime. Getting more after 16 seconds...\n",
      "Got 39 tweets for #TNFonPrime. Getting more after 12 seconds...\n",
      "Got 59 tweets for #TNFonPrime\n",
      "Got 39 unique tweets for #TNFonPrime from 59\n"
     ]
    }
   ],
   "source": [
    "## Getting tweets for each hashtag\n",
    "\n",
    "from twikit import Client\n",
    "import time\n",
    "from configparser import ConfigParser\n",
    "from random import randint\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "COOKIES_FILE = './cookies.json'\n",
    "\n",
    "async def get_top_tweets_for_trend(trend, max_count = 50):\n",
    "    '''\n",
    "    Get the top tweets for a given trend\n",
    "    It will get the first 20 tweets and then keep getting more until max_count is reached\n",
    "    The function will wait for a random time between 5 and 10 seconds before getting the next set of tweets\n",
    "    to avoid getting blocked by Twitter\n",
    "    '''\n",
    "    print(f'Getting tweets for {trend}')\n",
    "    all_tweets = []\n",
    "    # first set of tweets\n",
    "    tweets = await client.search_tweet(trend, 'Top', count = 20)\n",
    "    if (not tweets):\n",
    "        print(f'No tweets found for {trend}')\n",
    "        return all_tweets\n",
    "    all_tweets.extend(tweets)\n",
    "\n",
    "    while len(all_tweets) < max_count:\n",
    "        # get the next set of tweets\n",
    "        wait_time = randint(5, 20)\n",
    "        print(f'Got {len(all_tweets)} tweets for {trend}. Getting more after {wait_time} seconds...')\n",
    "        time.sleep(wait_time)\n",
    "        next_set = await tweets.next()\n",
    "        if (not next_set):\n",
    "            break\n",
    "        all_tweets.extend(next_set)\n",
    "\n",
    "    print(f'Got {len(all_tweets)} tweets for {trend}')\n",
    "    return all_tweets\n",
    "\n",
    "# Read the config file for credentials\n",
    "config = ConfigParser()\n",
    "config.read('../config.ini')\n",
    "username = config['X']['username']\n",
    "password = config['X']['password']\n",
    "email = config['X']['email']\n",
    "\n",
    "# authenticate with X.com\n",
    "# if 'cookies.json' is not present, then try to login, else use the cookies\n",
    "client = Client('en-US')\n",
    "if os.path.exists(COOKIES_FILE):\n",
    "    client.load_cookies(COOKIES_FILE)\n",
    "else:\n",
    "    await client.login(\n",
    "        auth_info_1 = username,\n",
    "        auth_info_2 = email,\n",
    "        password = password)\n",
    "    client.save_cookies(COOKIES_FILE)\n",
    "\n",
    "\n",
    "# Get the top 50 tweets for each hashtag\n",
    "# and save the tweets in a CSV file\n",
    "for index in range(0, len(all_hashtags)):\n",
    "    wait_time = randint(20, 40)\n",
    "    trend = all_hashtags[index]\n",
    "    print(f'{index}. Waiting for {wait_time} seconds before fetching tweets for {trend}...')\n",
    "    time.sleep(wait_time)\n",
    "\n",
    "    all_tweets = await get_top_tweets_for_trend(trend, 50)\n",
    "    twitter_texts= [tweet.text for tweet in all_tweets]\n",
    "    twitter_hashtags = [tweet.hashtags for tweet in all_tweets]\n",
    "    twitter_langs = [tweet.lang for tweet in all_tweets]\n",
    "    temp_df = pd.DataFrame({'text': twitter_texts, 'all_hashtags': twitter_hashtags, 'lang': twitter_langs})\n",
    "    # remove duplicates\n",
    "    temp_df = temp_df.drop_duplicates(subset='text')\n",
    "    temp_df['hashtag'] = trend\n",
    "    print(f'Got {len(temp_df)} unique tweets for {trend} from {len(all_tweets)}')\n",
    "\n",
    "    temp_df.to_csv(f'./data/{index}_{trend}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Highlights and Key Learnings**\n",
    "1. **Efficient Web Scraping**:\n",
    "   - Leveraged BeautifulSoup for precise extraction.\n",
    "   - Focused on reusable code to scrape multiple sections.\n",
    "\n",
    "2. **Respecting API Guidelines**:\n",
    "   - Random delays between API calls minimized the risk of being blocked.\n",
    "   - Ensured compliance with Twitter's rate limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "[Data Preprocessing](./02_DataPreprocessing.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
